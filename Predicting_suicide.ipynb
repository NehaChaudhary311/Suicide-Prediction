{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gaura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello this is neha how are you doing my email is neha gmail com yes \n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    #returns the string obtained by replacing HTML tags to \"\" i.e., remove the HTML tags\n",
    "    \n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    #re.search() to find the first match for a pattern. findall() finds *all* the matches and returns them as a list of strings\n",
    "    #remove all the non-word characters\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    #convert the text to lowercase\n",
    "    text = re.sub('[\\W]+', ' ', text.lower())\n",
    "    \n",
    "    text = text+' '.join(emoticons).replace('-', '') \n",
    "    return text\n",
    "\n",
    "text = \"Hello, this is Neha. How are you doing? My email is neha@gmail.com:: Yes!!\"\n",
    "print(preprocess_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 9119/9119 [00:00<00:00, 14029.49it/s]\n"
     ]
    }
   ],
   "source": [
    "#Applying the preprocess_text function to our suicidal dataset\n",
    "#Using tdqm : percentage of progress made in accomplishing a task.\n",
    "tqdm.pandas()\n",
    "df = pd.read_csv('suicidal_data.csv')\n",
    "df['tweet'] = df['tweet'].progress_apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the text by word using split()\n",
    "#Word stemming performed to convert the word to its root form\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords in the text\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'like', 'love', 'I', 'love', 'peopl', 'love', 'themselv']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing on a sentence\n",
    "[w for w in tokenizer_porter('I like loving myself and I love people who have loved themselves') if w not in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the hashing vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convert a collection of text documents to a matrix of token occurrences. If your are looking to get term frequencies weighted by their relative importance (IDF) then Tfidfvectorizer is what you should use. If you need the raw counts or normalized counts (term frequency), then you should use CountVectorizer or HashingVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\(|D|P)',text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower())\n",
    "    text += ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in tokenizer_porter(text) if w not in stop]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "#Hashing vectorizer object\n",
    "vect = HashingVectorizer(decode_error='ignore', n_features=2**21, \n",
    "                         preprocessor=None,tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stochastic gradient descent classifier algorithm.\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss='log', random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df[\"tweet\"].to_list()\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting our dataset - 80% for training, 20% for testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,\n",
    "                                                 y,\n",
    "                                                 test_size=0.20,\n",
    "                                                 random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming the text data to vectors with the Hashing Vectorizer we created earlier\n",
    "X_train = vect.transform(X_train)\n",
    "X_test = vect.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "              l1_ratio=0.15, learning_rate='optimal', loss='log', max_iter=1000,\n",
       "              n_iter_no_change=5, n_jobs=None, penalty='l2', power_t=0.5,\n",
       "              random_state=1, shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the data to the algorithm\n",
    "classes = np.array([0, 1])\n",
    "clf.partial_fit(X_train, y_train,classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.907\n"
     ]
    }
   ],
   "source": [
    "#Accuracy test on our dataset\n",
    "print('Accuracy: %.3f' % clf.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updating the model with prediction\n",
    "clf = clf.partial_fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Predicting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: negative\n",
      "Probability: 51.33%\n"
     ]
    }
   ],
   "source": [
    "label = {0:'negative', 1:'positive'}\n",
    "example = [\"I'm going to die and I hate my life as fuck\"]\n",
    "X = vect.transform(example)\n",
    "print('Prediction: %s\\nProbability: %.2f%%'\n",
    "      %(label[clf.predict(X)[0]],np.max(clf.predict_proba(X))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: positive\n",
      "Probability: 86.94%\n"
     ]
    }
   ],
   "source": [
    "label = {0:'negative', 1:'positive'}\n",
    "example = [\"I'm depressed and lonely.... Please help me.\"]\n",
    "X = vect.transform(example)\n",
    "print('Prediction: %s\\nProbability: %.2f%%'\n",
    "      %(label[clf.predict(X)[0]],np.max(clf.predict_proba(X))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: negative\n",
      "Probability: 92.28%\n"
     ]
    }
   ],
   "source": [
    "label = {0:'negative', 1:'positive'}\n",
    "example = [\"I want to sleep today.\"]\n",
    "X = vect.transform(example)\n",
    "print('Prediction: %s\\nProbability: %.2f%%'\n",
    "      %(label[clf.predict(X)[0]],np.max(clf.predict_proba(X))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
